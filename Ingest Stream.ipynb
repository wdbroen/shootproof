{"cells":[{"cell_type":"markdown","source":["This notebook ingests the last 2 days of photon stream, transforms it, and writes it back out as parquet. There are two versions of the schema in the stream which can be identified by inspecting the record."],"metadata":{}},{"cell_type":"code","source":["spark.table('photon.photon_raw_partitioned').printSchema()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import datetime as dt\nnumber_of_days_to_load = 3\ndef days_to_load_fuction():\n  return range(0,number_of_days_to_load)[-1]*(-1)\n\nsqlContext.registerFunction('days_to_load',days_to_load_fuction)\n\nfiles_to_load = []\n\nfor days in range(0,number_of_days_to_load):\n  file_date = dt.date.today()+dt.timedelta(days=-1*days)\n  file = \"mnt/cc-data-lake/1-raw/photon/raw/%s/%s/%s/*\" % (file_date.year,file_date.strftime('%m'), file_date.strftime('%d'))\n  files_to_load.append(file)\n  \nfiles_to_load"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndf = spark.read.json(files_to_load).withColumn(\"date\", to_date(col(\"addedUtc\")))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom datetime import *\nfrom hashlib import sha256\nfrom k2databricks import k2\nimport os, re\n\n\ndef merge_stream_into_table(input_df, output_table, dedupe_columns, validations, rows_per_split):\n  \"\"\"\n    Takes input data that may span multiple target partitions, dedupes it against existing data in the partitions, and\n    loads the diff into the partitions\n  \"\"\"\n  def write_log(s):\n    print(\"%s %s: %s\" % (datetime.now(), output_table, s))\n  \n  input_df = input_df.cache()\n  input_row_count = input_df.count()\n  # find unique list of dates in the input stream, to determine which target partitions data will be loaded into.\n  partition_dates = input_df.selectExpr(\"collect_list(DISTINCT date)\").collect()[0][0]\n  write_log(\"Found %s input records for partit0ions %s\" % (input_row_count, \", \".join([str(d) for d in sorted(partition_dates)])))\n\n  #validate & scrub the input data\n  valid_df = input_df\n  for validation in validations:\n    valid_df = valid_df.filter(validation)\n\n  valid_row_count = valid_df.count()\n  write_log(\"Data validation scrubbed %s records\" % (input_row_count - valid_row_count))\n\n  # dedupe the input against itself first\n  dedupe_df = valid_df.dropDuplicates(dedupe_columns)\n  #dedupe_df = input_df\n  dedupe_row_count = dedupe_df.count()\n  write_log(\"Deduped %s records within input stream\" % (valid_row_count - dedupe_row_count))\n  \n  # load any existing data from the target partitions, then dedupe the input against it\n  existing_df = spark.table(output_table).filter(col(\"date\").isin(partition_dates))\n  delta_df = dedupe_df \\\n    .join(existing_df, dedupe_columns, \"leftanti\") \\\n    .selectExpr(existing_df.columns) \\\n    .cache()\n  \n  output_row_count = delta_df.count()\n  delta_row_count = dedupe_row_count - output_row_count\n  write_log(\"Deduped %s records against existing partition data\" % (delta_row_count))\n  if output_row_count:\n    # compute the number of splits based upon how many rows remain following the dedupe process\n    num_splits = 1 + output_row_count / rows_per_split\n    write_log(\"Loading %s records using %s splits\" % (output_row_count, num_splits))\n    \n    spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n    delta_df.repartition(num_splits).write.mode(\"append\").save(output_table)\n  else:\n    write_log(\"No new records to load after dedupe was performed\")\n    \n  input_df.unpersist()\n  delta_df.unpersist()\n  \n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["merge_stream_into_table(df, 'photon.photon_raw_partitioned', ['addedUtc','cookieId'], [], 100000)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport urlparse\nimport httpagentparser\n\ndef parse_query_string(url):\n  query_string = urlparse.urlsplit(url).query\n  return dict(urlparse.parse_qsl(query_string))\n\ndef parse_domain(url):\n  try:\n    return urlparse.urlparse(url).netloc\n  except:\n    return None\n\ndef parse_user_agent(ua):\n  parsed = httpagentparser.detect(ua)\n  return {\n    \"bot\": parsed.get(\"bot\"),\n    \"browser\": parsed.get(\"browser\"),\n    \"os\": parsed.get(\"platform\")\n  }\n\nparse_user_agent_schema = StructType([\n  StructField(\"bot\", BooleanType()),\n  StructField(\"browser\", StructType([\n    StructField(\"name\", StringType()),\n    StructField(\"version\", StringType())\n  ])),\n  StructField(\"os\", StructType([\n    StructField(\"name\", StringType()),\n    StructField(\"version\", StringType())\n  ])),\n])\n\nsqlContext.registerFunction(\"parse_query_string\", parse_query_string, MapType(StringType(), StringType()))\nsqlContext.registerFunction(\"parse_domain\", parse_domain, StringType())\nsqlContext.registerFunction(\"parse_user_agent\", parse_user_agent, parse_user_agent_schema)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW photon\nAS\n\nWITH\nphoton_live_hits AS (\n  SELECT\n    * \n  FROM photon.photon_raw_partitioned\n  WHERE type = 'hit'\n  --AND date(addedUtc) >= '2018-07-01'\n  AND date(addedUtc) >= date_add(current_date,days_to_load())\n),\n\nphoton_v1 AS (\n  SELECT * FROM photon_live_hits WHERE data.url IS NOT NULL\n),\n\nphoton_v2 AS (\n  SELECT * FROM photon_live_hits WHERE data.url IS NULL AND data.serverReferrer IS NOT NULL\n),\n\nnormalized AS (\n  SELECT\n    addedUtc,\n    cookieId,\n    struct(\n      data.url AS page_url,\n      boolean(NULL) AS page_url_verified_by_photon,\n      string(NULL) AS referring_url,\n      string(NULL) AS full_url,\n      parse_query_string(data.url) AS query_params,\n      parse_domain(data.url) AS domain\n    ) AS url,\n    parse_user_agent(data.userAgent) AS parsed_user_agent,\n    data\n  FROM photon_v1\n\n  UNION ALL\n\n  SELECT\n    addedUtc,\n    cookieId,\n    struct(\n      data.serverReferrer AS page_url,\n      data.urlVerified AS page_url_verified_by_photon,\n      data.clientReferrer AS referring_url,\n      data.clientUrl AS full_url,\n      parse_query_string(data.serverReferrer) AS query_params,\n      parse_domain(data.serverReferrer) AS domain\n    ) AS url,\n    parse_user_agent(data.userAgent) AS parsed_user_agent,\n    data\n  FROM photon_v2\n),\n\ntransformed AS (\n  SELECT\n    date(addedUtc) AS date,\n    addedUtc AS activity_time,\n    cookieId AS cc_cookie_id,\n    data.ip AS ip,\n    url,\n    struct(\n      data.platform,\n      data.colorDepth AS color_depth,\n      data.screenHeight AS screen_height,\n      data.screenWidth AS screen_width,\n      parsed_user_agent.bot AS self_identified_as_bot,\n      parsed_user_agent.browser,\n      parsed_user_agent.os\n    ) AS device,\n    struct(\n      url.query_params[\"utm_campaign\"] AS campaign,\n      url.query_params[\"utm_source\"] AS source,\n      url.query_params[\"utm_content\"] AS content,\n      url.query_params[\"utm_medium\"] AS medium,\n      url.query_params[\"utm_term\"] AS term\n    ) AS utm,\n    data\n  FROM normalized\n)\n\nSELECT * FROM transformed;"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#old code to write the entire table\n#spark.table(\"photon\").write.partitionBy(\"date\").saveAsTable(\"photon.photon\", format=\"orc\", mode=\"overwrite\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["merge_stream_into_table(spark.table('photon'), 'photon.photon', ['activity_time','cc_cookie_id'], [], 100000)\n#merge_stream_into_table(input_df, output_table, dedupe_columns, validations, rows_per_split)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"Ingest Stream","notebookId":86227},"nbformat":4,"nbformat_minor":0}
